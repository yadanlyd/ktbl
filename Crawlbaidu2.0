import urllib.request
from bs4 import BeautifulSoup
import pymysql
import re
import time
import pickle


class Crawlbaidu():
    
    rkey = ['科研申报'] #存放已经爬取过的相关搜索关键字
    dedup = [] #保存第二次过滤链接，用于去重
    rURL = [("http://www.baidu.com/s?wd=%E7%A7%91%E7%A0%94%E7%94%B3%E6%8A%A5&tn=94739417_hao_pg&ssl_sample=hao_1","科研申报")] #保存第二次过滤百度域内的结果

    net = "http://www.baidu.com"

    index=[0]
    
    # 过滤词
    stopWords = ['公示','名单','结项','防范','评选','调查','结果','撤项','调整','会议','召开','评估','终止',
                 '立项','邀请','百度','如何','模板','怎样','注意事项','提供','辅导','收费','博客','豆丁','新浪'
                 '道客巴巴','范文','软件','攻略']
    chooseWords = ['申报','通知','重大','项目','指南','通告','招标','课题','征集','选题','平台','信息','科学部','科学技术部']


    def __init__(self,page_total,url,flag,conn,cur):
        #第一个参数是需要获取百度结果的总页数
        #第二个参数第一次爬取的链接
        #第三个参数是去重功能的参数，默认输入1
        #第四、五个参数是数据库需要传入的变量
        
        self.page_total = page_total #获取链接的总页数  
        self.page_num = 0 #当前运行的页码  
        self.url =url # 第一次爬取的链接
        
        self.all_url = [] #所有a标签下的url
        self.all_title = [] #所有a标签下的url的标题
        self.qURL = [] #保存第一次词库过滤结果
        self.dURL = [] #保存第二次过滤非百度域内的结果
        

        self.flag = flag #去重的参数
        self.conn = conn
        self.cur = cur
    
    def visitBaidu(self):
        # 从百度获取所有链接
        k = self.page_num
        for i in range(k,self.page_total):
            # 爬取本页的源代码
            try:
                page = "&pn="+str(10*i)
                if i==0:
                    page = ""
                url=self.url+page
                data=urllib.request.urlopen(url).read()

                # 提取网址和标题
                soup = BeautifulSoup(data, "lxml")
                for link in soup.find_all('a'):
                    self.all_url.append(link.get('href'))
                    self.all_title.append(link.get_text())
                self.page_num+=1
            except Exception:
                continue

    def Filtration(self):
        # 第一次过滤：符合词库条件的链接压入列表
        for i in range(len(self.all_url)):
            url = self.all_url[i]
            title = self.all_title[i]
            stopword_appear,chooseword_appear = 0,0
            for s in self.stopWords:
                if s in title:
                     stopword_appear+=1
            for c in self.chooseWords:
                if c in title:
                    chooseword_appear+=1
             # 测试结果是选择词大于2，准确率会更高
            if chooseword_appear>=1 and stopword_appear<=0:
                # 存入队列
                self.qURL.append((url, title))

    
    def DirectURL(self):
        #第二次过滤：将真正的网址保存在列表里
        for q in self.qURL:
            try:
                # rURL列表保存百度的相关搜索链接
                if "http" not in q[0] and "https" not in q[0]:
                    if q[1] not in self.rkey:
                        self.rURL.append((self.net+q[0],q[1]))
                        self.rkey.append(q[1])
                    
                # 找到第一次过滤的链接指向的真正的页面链接   
                u=urllib.request.urlopen(q[0])
                url=u.geturl()
                
                # 去掉仍是百度局域内的网址
                if "baidu.com" not in url:
                    self.dURL.append((url,q[1]))
                else:
                    # 若网址仍在百度域内，存在两种情况：
                    # 1、返回了请求的真正的网址（正则表达式匹配）；
                    # 2、返回页面源码（此类网址应删去）
                    data3=u.read()
                    p=re.compile(b'(?<=URL=\').*?(?=\'"></noscript>)')
                    a=p.search(data3)
                    if a:
                        self.dURL.append((a.group(0),q[1]))
            except Exception:
                continue

    
    def Deduplication(self):
        # 去重，将已经过滤好的网站，截取保留网站的首页链接
        for d in self.dURL:
            rex = re.compile('http://.*?/|https://.*?/')
            aex = rex.search(d[0])
            # 功能一：按url去重
            if self.flag==1:
                t = aex.group(0)
                tempurl = []
                for de in self.dedup:
                    tempurl.append(de[0])
                if t not in tempurl:
                    self.dedup.append((t,d[1]))
            # 功能二：按url、title共同去重
            elif self.flag==2:
                t = (aex.group(0),d[1])
                if t not in self.dedup:
                    self.dedup.append(t)

    def Savesql(self):
        #存入数据库
        dlen = len(self.dedup)
        i=self.index[0]
        for d in range(i,dlen):
            param = (d , self.dedup[d][0] , self.dedup[d][1])
            sql = "insert into ktbl (url_id,url,title) values(%s,%s,%s)"
            self.cur.execute(sql, param)
            self.conn.commit()
            self.index[0]=self.index[0]+1

    
            
    def Startcrawl(self):
        # 启动函数
        self.visitBaidu()
        print("1")
        self.Filtration()
        print("2")
        self.DirectURL()
        print("3")
        self.Deduplication()
        print("4")
        # self.Savesql()
        # print("5")


#数据库链接
conn= pymysql.connect(
        host='localhost',
        port=3306,
        user='root',
        passwd='009795',
        db='pythonsql',
        charset='utf8',
        cursorclass=pymysql.cursors.DictCursor,
        )
cur = conn.cursor()

start = time.clock()

count=0
# 从百度的相关链接列表中依次对每一个链接进行爬取
for r in Crawlbaidu.rURL:
    count+=1
    # 越到后面的网站，关联度越低，所以打开的总页面数应减少
    s = Crawlbaidu(5,r[0],1,conn,cur)
    s.Startcrawl()
    # 此循环不会停止，count用于测试时结束循环
    print(count)
    if count==1:
        break

end = time.clock()
print("用时为：",end - start,'s')

conn.close()

    
        
    
