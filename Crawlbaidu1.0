'''
1、翻页（GET）
2、获取每一条连接，从标题判断是否为申报信息
3、若是，判断是否在数据库中，若为新连接则加入数据库中
4、队列，广度优先搜索
'''
'''
问题：
1、第一页无法获取源代码？(已解决）
2、每一次爬取的总数不一？（已解决，百度的搜索结果会改变）
3、打开搜索结果链接的部分偶尔会报错？（加异常处理）
'''
import urllib.request
from bs4 import BeautifulSoup
import pymysql
import re
import time
import pickle

# 过滤词
stopWords = ['公示','名单','结项','防范','评选','调查','结果','撤项','调整','会议','召开','评估','终止',
             '立项','邀请','百度','如何','模板','怎样','注意事项','提供','辅导','收费','博客','豆丁','新浪'
             '道客巴巴','范文','软件','攻略']
chooseWords = ['申报','通知','重大','项目','指南','通告','招标','课题','征集','选题','平台','信息','科学部','科学技术部']

#全局变量，列表代替队列，方便去重
qURL = []#保存第一次过滤
dURL = []#保存第二次过滤
dedup = []#保存第二次过滤链接，用于去重
rURL = []#存放相关搜索的链接
rkey = ['科研申报']#存放已经

找到过的相关搜索关键字

index = 0
flag = 1 #去重的参数
start_url = "http://www.baidu.com/s?wd=%E7%A7%91%E7%A0%94%E7%94%B3%E6%8A%A5&tn=94739417_hao_pg&ssl_sample=hao_1"
net = "http://www.baidu.com"
all_tag_a_url = []
all_tag_a_title = []

# 若需要压进前1000条搜索结果，则allpage = 100，3为测试数字
# 总页数page = 1320000
#allpage = 1

def visitBaidu(allpage,url):
    # 从百度获取所有链接
    for i in range(0,allpage):
        # 爬取本页的源代码
        try:
            page = "&pn="+str(10*i)
            if i==0:
                page = ""
            url=url+page
            data=urllib.request.urlopen(url).read()
            #path = "D:/page_is_" +str(i+1)+".txt"
            #f = open(path,"wb")
            #f.write(data)
            #f.close()

            # 提取网址和标题
            global all_tag_a_url,all_tag_a_title
            soup = BeautifulSoup(data, "lxml")
            for link in soup.find_all('a'):
                all_tag_a_url.append(link.get('href'))
                all_tag_a_title.append(link.get_text())
        except Exception:
            continue



def Filtration():
    # 第一次过滤：符合词库条件的链接压入列表
    for i in range(len(all_tag_a_url)):
        global qURL
        a_url = all_tag_a_url[i]
        a_title = all_tag_a_title[i]
        stopword_appear,chooseword_appear = 0,0
        for s in stopWords:
            if s in a_title:
                 stopword_appear+=1
        for c in chooseWords:
            if c in a_title:
                chooseword_appear+=1
         # 测试结果是选择词大于2，准确率会更高
        if chooseword_appear>=1 and stopword_appear<=0:
            # 存入队列
            qURL.append((a_url, a_title))


def DirectURL():
    #第二次过滤：将真正的网址保存在列表里
    for q in qURL:
        try:
            # rURL列表保存百度的相关搜索链接
            if "http" not in q[0] and "https" not in q[0]:
                rURL.append((net+q[0],q[1]))
                
            # 找到第一次过滤的链接指向的真正的页面链接   
            u=urllib.request.urlopen(q[0])
            url=u.geturl()
            
            # 去掉仍是百度局域内的网址
            if "baidu.com" not in url:
                dURL.append((url,q[1]))
            else:
                # 若网址仍在百度域内，存在两种情况：
                # 1、返回了请求的真正的网址（正则表达式匹配）；
                # 2、返回页面源码（此类网址应删去）
                data3=u.read()
                p=re.compile(b'(?<=URL=\').*?(?=\'"></noscript>)')
                a=p.search(data3)
                if a:
                    dURL.append((a.group(0),q[1]))
        except Exception:
            continue

        
def Deduplication(flag):
    global dedup
    # 二次去重，将已经过滤好的网站，只保留网站的首页链接
    for d in dURL:
        rex = re.compile('http://.*?/|https://.*?/')
        #print(d[0])
        aex = rex.search(d[0])
        # 功能一：按url去重
        if flag==1:
            t = aex.group(0)
            tempurl = []
            for de in dedup:
                tempurl.append(de[0])
            if t not in tempurl:
                dedup.append((t,d[1]))
        # 功能二：按url、title共同去重
        elif flag==2:
            t = (aex.group(0),d[1])
            if t not in dedup:
                dedup.append(t)
                
            
def Savesql():
    #存入数据库
    #数据库链接
    conn= pymysql.connect(
            host='localhost',
            port=3306,
            user='root',
            passwd='009795',
            db='pythonsql',
            charset='utf8',
            cursorclass=pymysql.cursors.DictCursor,
        )
    cur = conn.cursor()
    global index
    dlen = len(dedup)
    for d in range(index,dlen):
        param = (d+1,dedup[d][0],dedup[d][1])
        sql = "insert into ktbl (url_id,url,title) values(%s,%s,%s)"
        cur.execute(sql, param)
        conn.commit()
    index+=len(dedup)
    cur.close()
    conn.close()


def findNewurl():
    # 从队列中取出URL，继续查找
    # qURL_len设置成qURL的长度，迭代会不断的往qURL末尾添加新链接，3为测试数字
    # qURL_len = len(qURL)
    for i in range(0,len(dURL)):
        data2 = urllib.request.urlopen(dURL[i][0]).read()
        all_tag_a_url, all_tag_a_title = getURL(data2)
        Filtration(all_tag_a_url, all_tag_a_title)
        Savesql()


def Relevantsearch():
    #从相关搜索链接继续查找

    count=0
    for r in rURL:
        if r[1] in rkey:
            continue
        all_tag_a_url.clear()
        all_tag_a_title.clear()
        visitBaidu(3,r[0])

        
        qURL.clear()
        Filtration()


        dURL.clear()
        DirectURL()

        Deduplication(flag)

        Savesql()
        rkey.append(r[1])

        #rURL会不断地增加，循环不会停止
        #计数便于跳出循环
        if count==10:
            break
        count+=1
        



#从百度获取所有链接
start1 = time.clock()
visitBaidu(1,start_url)
end1 = time.clock()

#第一次过滤,符合条件的压入队列
start2 = time.clock()
#Filtration()
end2 = time.clock()

#第二次过滤，
start3 = time.clock()
DirectURL()
end3 = time.clock()

#第二次去重，取首页
Deduplication(flag)
    
#存入数据库
start4 = time.clock()
#Savesql()
end4 = time.clock()

#以百度里的相关搜索为关键字，继续查找
#Relevantsearch()

# 输出：


'''
print("获取百度所有链接用时：",end1 - start1,'s')
print("第一次过滤用时：",end2 - start2,'s')
print("第二次过滤用时：",end3 - start3,'s')
print("存入数据库用时：",end4 - start4,'s')
'''




